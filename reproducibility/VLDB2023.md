This page contains the information required to reproduce the experimental results of our research paper 
*Similarity search in the blink of an eye with compressed indices* [[ABHT23]](#1) published at VLDB 2023 ([arxiv version](https://arxiv.org/abs/2304.04759)).

## Protocol and Code
We conduct an exhaustive evaluation showing that our optimized graph-based search and LVQ (OG-LVQ), establishes a new 
SOTA for both small and large scale datasets. For the small and large scale experiments we adopt the 
[ANN-benchmarks](https://ann-benchmarks.com/) and [Big-ANN-benchmarks](https://github.com/harsha-simhadri/big-ann-benchmarks) 
protocols respectively.

The **code to run OG-LVQ** can be found [here](https://github.com/IntelLabs/ScalableVectorSearch), and it was included in the 
ANN-benchmarks and Big-ANN-benchmarks evaluation codes following their 
[guidelines](https://github.com/erikbern/ann-benchmarks/#including-your-algorithm). See the corresponding sections below
for details on the code and setup used for the compared implementations at [small](#small-scale-experiments) and 
[large](#large-scale-experiments) scale.

## Datasets and Metrics
To cover a wide range of use cases, we evaluate OG-LVQ on standard datasets of diverse dimensionalities, number of 
elements, data types and metrics as described in the table below.

| **Dataset**                                                                                         | **d** | **n** | **Encoding** | **Similarity**    | **n queries** | **Space (GiB)** |
|-----------------------------------------------------------------------------------------------------|-------|-------|--------------|-------------------|---------------|-----------------|
| [gist-960-1M](http://corpus-texmex.irisa.fr/)                                                | 960   | 1M    | float32      | L2                | 1000          | 3.6             |
| [sift-128-1M](http://corpus-texmex.irisa.fr/)                                                   | 128   | 1M    | float32      | L2                | 10000         | 0.5             |
| [deep-96-10M](http://sites.skoltech.ru/compvision/noimi/)                                     | 96    | 10M   | float32      | cosine similarity | 10000         | 3.6             |
| [glove-50-1.2M](https://nlp.stanford.edu/projects/glove/)                                       | 50    | 1.2M  | float32      | cosine similarity | 10000         | 0.2             |
| [glove-25-1.2M](https://nlp.stanford.edu/projects/glove/)                                       | 25    | 1.2M  | float32      | cosine similarity | 10000         | 0.1             |
| [DPR-768-10M](https://github.com/IntelLabs/DPR-dataset-generator)                                                                    | 768   | 10M   | float32      | inner product     | 10000         | 28.6            |
| [t2i-200-100M](https://research.yandex.com/blog/benchmarks-for-billion-scale-similarity-search) | 200   | 100M  | float32      | inner product     | 10000         | 74.5            |
| [deep-96-100M](http://sites.skoltech.ru/compvision/noimi/)                                     | 96    | 100M  | float32      | cosine similarity | 10000         | 35.8            |
| [deep-96-1B](http://sites.skoltech.ru/compvision/noimi/)                                       | 96    | 1B    | float32      | cosine similarity | 10000         | 257.6           |
| [sift-128-1B](http://corpus-texmex.irisa.fr/)                                                  | 128   | 1B    | uint8        | L2                | 10000         | 119.2           |


### DPR-768-10M Dataset

DPR is a dataset containing 10 million 768-dimensional embeddings generated with the dense passage retriever (DPR) 
[[KOML20]](#2) model. Text snippets from the C4 dataset [[RSRL20]](#3) are used to generate: 10 million context DPR embeddings
(base set) and 10000 question DPR embeddings (query set). 
The code to generate the dataset can be found [here](https://github.com/IntelLabs/DPR-dataset-generator).

### Evaluation Metrics

In all benchmarks and experimental results, search accuracy is measured by k-recall at k, defined by
$| S \cap G_t | / k$, where $S$ are the ids of the $k$ retrieved neighbors and
$G_t$ is the ground-truth. We use $k=10$ in all experiments (the [supplementary material](https://arxiv.org/abs/2304.04759) 
also includes results for $k=50$ and $k=100$).
Search performance is measured by queries per second (QPS).

### Query Batch Size
The size of the query batch, which will depend on the use case, can have a big impact on performance. Therefore,
we evaluate batch sizes: 1 (one query at a time or single query) and full batch 
(determined by the number of queries in the dataset).

## Small Scale Experiments
### System Setup and Datasets
We run the small scale experiments on a 3rd generation Intel® Xeon® Platinum 8360Y @2.40GHz with
36 cores (single socket), equipped with 256GB DDR4 memory per socket @2933MT/s speed, running Ubuntu 22.04.[^1]

We ran all experiments in a single socket (using ``numactl``) to avoid introducing performance regressions due to
remote NUMA memory accesses.

We consider datasets of diverse dimensionalities and number of vectors (see [Datasets and Metrics](#datasets-and-metrics) 
for details about these datasets):

* glove-25-1.2M (25 dimensions, 1 million points)
* glove-50-1.2M (50 dimensions, 1 million points)
* deep-96-10M (96 dimensions, 10 million points)
* sift-128-1M (128 dimensions, 1 million points)
* gist-960-1.0M (960 dimensions, 1 million points)

### Comparison to Other Implementations
We compare OG-LVQ to five widely adopted approaches: Vamana [[SDSK19]](#4), HSNWlib [[MaYa18]](#5), FAISS-IVFPQfs 
[[JoDJ19]](#6), ScaNN [[GSLG20]](#7), and NGT [[IwMi18]](#8). We use the implementations available through [ANN-Benchmarks](https://github.com/erikbern/ann-benchmarks)
([commit 167f129](https://github.com/erikbern/ann-benchmarks/commit/167f1297b21789d13a9fa82646c522011df8c163) , October 4th 2022)
and for OG-LVQ we use [commit ad821d8](https://github.com/IntelLabs/ScalableVectorSearch/commit/ad821d8c94cb69a67c8744b98ee1c79d3e3a299c).

#### Parameters Setting
For the graph-based methods (HSNWlib, Vamana, OG-LVQ) we use the same ``graph_max_degree`` values (32, 64 and 128).
For IVFPQfs, ScaNN and NGT-qg we consider the provided [yaml configuration files](https://github.com/erikbern/ann-benchmarks/).
For OG-LVQ, we include various LVQ settings (LVQ-8, LVQ-4x4, LVQ-4x8, and LVQ8x8) as well as float16 and float32 encodings.
LVQ-compressed vectors are padded to half cache lines (``padding``= 32).


## Large Scale Experiments
### System Setup and Datasets
We run our experiments on a 3rd generation Intel® Xeon® Platinum 8380 CPU @2.30GHz with
40 cores (single socket), equipped with 1TB DDR4 memory per socket @3200MT/s speed,  running Ubuntu 22.04.[^1]

We ran all experiments in a single socket (using ``numactl``) to avoid introducing performance regressions due to
remote NUMA memory accesses.

We use the ``hugeadm`` Linux utility to preallocate a sufficient number of 1GB huge pages for each algorithm.
[OG-LVQ explicitly uses huge pages](https://intellabs.github.io/ScalableVectorSearch/performance/hugepages.html) to reduce the virtual memory overheads.
For a fair comparison, we run other methods with system flags enabled to automatically use huge pages for large allocations.

We consider datasets that are large scale because of their total footprint (see [Datasets and Metrics](#datasets-and-metrics)
for details about these datasets):

* deep-96-1B (96 dimensions, 1 billion points)
* sift-128-1B (128 dimensions, 1 billion points)
* t2i-200-100M (200 dimensions, 100 million points)
* DPR-768-10M (768 dimensions, 10 million points)

### Comparison to Other Implementations
We compare the performance of OG-LVQ vs. four widely adopted approaches: Vamana [[SDSK19]](#4), HSNWlib [[MaYa18]](#5), FAISS-IVFPQfs
[[JoDJ19]](#6), and ScaNN [[GSLG20]](#7).

We used the following versions of each method:
OG-LVQ [commit ad821d8](https://github.com/IntelLabs/ScalableVectorSearch/commit/ad821d8c94cb69a67c8744b98ee1c79d3e3a299c),
Vamana [commit 647f68f](https://github.com/microsoft/DiskANN/commit/647f68fe5aa7b45124ae298c219fe909d46a1834),
HNSWlib [commmit 4b2cb72](https://github.com/nmslib/hnswlib/commit/4b2cb72c3c1bbddee55535ec6f360a0b2e40a81e),
ScaNN [commit d170ac5](https://github.com/google-research/google-research/commit/d170ac58ce1d071614b2813b056afa292f5e490c),
and FAISS-IVFPQfs [commit 19f7696](https://github.com/facebookresearch/faiss/commit/19f7696deedc93615c3ee0ff4de22284b53e0243).

### Parameter Settings
#### Search with Reduced Memory Footprint
In large-scale scenarios, the memory requirement grows quickly, in particular for graph-based methods. This makes these
methods expensive, as the system cost is dominated by the total DRAM price. We compare here the performance of the 
considered approaches under different memory footprint regimes.

For **OG-LVQ** and **Vamana**, we build Vamana graphs with: ``graph_max_degree`` = 32, 64, 126 and ``alpha`` = 1.2. OG-LVQ
is used with LVQ-8 compressed vectors, with vectors padded to half cache lines (``padding`` = 32).
For **HSNWlib**, we build graphs with a window search size of 200 and ``graph_max_degree`` = 32, 64, 96 (this corresponds
to M=16, 32, 48 in HSNW notation). We had to reduce ``graph_max_degree`` from 128 to 96 to fit the working set size in
1TB memory.

For **FAISS-IVFPQfs**, we use ``nlist`` = 32768 and ``nbins`` = 48.
Re-ranking is enabled, and at runtime we sweep ``nprobe`` = [1,5,10,50,100,20] and  ``k for re-ranking`` = [0,10,100,1000].
For **ScaNN**, we use the recommended parameters setting: ``n_leaves`` = $\sqrt{n}$, ``avq_threshold`` = 0.2,
``dims_per_block`` = 2 (where $n$ is the number of vectors in the dataset), as that is the best among several
evaluated settings and vary the runtime parameters (``leaves_to_search`` = [2-1000], ``reorder`` = [20-1000]).
For FAISS-IVFPQfs and ScaNN, which follow the same index design, the memory footprint is almost constant for different
considered parameter settings.

#### High-throughput Regime
In the high-throughput regime all methods are set assuming high throughput is the main priority and memory availability
is not a major issue. 

For **OG-LVQ** and **Vamana**, we use the following parameter setting to build Vamana graphs 
for all the datasets:

* ``graph_max_degree`` = 128 (we use ``graph_max_degree`` = 126 for deep-96-1B),
* ``alpha`` = 1.2 and ``alpha`` =  0.95 for Euclidean distance and inner product respectively.

For OG-LVQ, we include various LVQ settings (LVQ-8, LVQ-4x4, LVQ-4x8, and LVQ8x8) as well as float16 and float32 encodings.
LVQ-compressed vectors are padded to half cache lines (``padding`` = 32).

For **HSNWlib**, we build all graphs with a window search size of 200 and ``graph_max_degree`` = 128 (this corresponds
to M=64 in HSNW notation), except deep-96-1B for which we must reduce ``graph_max_degree`` to 96 to fit the
working set size in 1TB memory.

For **FAISS-IVFPQfs**, we pre-build indices with ``nlist`` = 32768 and ``nbins`` = $d/2$ (where $d$ is the dataset dimensionality)
for the 1 billion scale datasets deep-96-1B and sift-128-1B. For t2i-200-100M and DPR-768-10M, indices are built on the fly
with combinations of ``nlist`` =[512, 1024, 4096, 8192] and ``nbins`` =[d/4, d/2, d].
Re-ranking is enabled, and at runtime we sweep ``nprobe`` =[1,5,10,50,100,20] and  ``k for re-ranking`` = [0,10,100,1000].

For **ScaNN**, we use the recommended parameters setting: ``n_leaves`` = $\sqrt{n}$, ``avq_threshold`` = 0.2,
``dims_per_block`` = 2 (where $n$ is the number of vectors in the dataset) for the billion scale datasets
(deep-96-1B and sift-128-1B), as that is the best among several evaluated settings. For t2i-200-100M and DPR-768-10M we evaluate
different parameter settings (see Table below). For all dataests we vary the runtime parameters
(``leaves_to_search`` = [2-1000], ``reorder`` = [20-1000]).

| **t2i-200-100M** |                   |                    | **DPR-768-10M** |                   |                    |
|------------------|-------------------|--------------------|-----------------|-------------------|--------------------|
| **n_leaves**     | **avq_threshold** | **dims_per_block** | **n_leaves**    | **avq_threshold** | **dims_per_block** |
| 2000             | 0.2               | 1                  | 1000            | 0.55              | 1                  |
| 5000             | 0.15              | 3                  | 2000            | 0.2               | 1                  |
| 10000            | 0.2               | 2                  | 3162            | 0.2               | 2                  |
| 20000            | 0.2               | 2                  | 6000            | 0.2               | 2                  |

In all cases where several parameter settings are evaluated, the results show the corresponding Pareto lines.

## References

<a id="1">[ABHT23]</a>
Aguerrebere, C.; Bhati I.; Hildebrand M.; Tepper M.; Willke T.:Similarity search in the blink of an eye with compressed
indices. In: Proceedings of the VLDB Endowment, 16, 11 (2023)

<a id="2">[KOML20]</a>  
Karpukhin, V.; Oguz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov, S.; Chen, D.; Yih, W..: Dense Passage Retrieval for Open-Domain Question Answering. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 6769–6781. (2020)

<a id="3">[RSRL20]</a>
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P.J.: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. In: The Journal of Machine Learning Research 21,140:1–140:67.(2020)

<a id="4">[SDSK19]</a>
Subramanya, S.J.; Devvrit, F.; Simhadri, H.V.; Krishnawamy, R.; Kadekodi, R..:Diskann: Fast accurate billion-point nearest neighbor search on a single node. In: Advances in Neural Information Processing Systems 32 (2019).

<a id="5">[MaYa18]</a>
Malkov, Y. A. and Yashunin, D. A..: Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. In: IEEE transactions on pattern analysis and machine intelligence 42, 4 (2018), 824–836.

<a id="6">[JoDJ19]</a>
Johnson, J.; Douze, M.; Jégou, H..: Billion-scale similarity search with GPUs. In: IEEE Transactions on Big Data 7, 3 (2019), 535–547.

<a id="7">[GSLG20]</a>
Guo, R.; Sun, P.; Lindgren, E.; Geng, Q.; Simcha, D.; Chern, F.; Kumar, S..: Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning. PMLR, 3887-3896 (2020)

<a id="8">[IwMi18]</a>
Iwasaki, M. and Miyazaki, D..: Nearest Neighbor Search with Neighborhood Graph and Tree for High-dimensional Data. https://github.com/yahoojapan/NGT (2018)


[^1]: Performance varies by use, configuration and other factors. Learn more at [www.Intel.com/PerformanceIndex](http://www.Intel.com/PerformanceIndex/).
Performance results are based on testing as of dates shown in configurations and may not reflect all publicly
available updates. No product or component can be absolutely secure. Your costs and results may vary. Intel
technologies may require enabled hardware, software or service activation. &copy; Intel Corporation.  Intel,
the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and
brands may be claimed as the property of others.